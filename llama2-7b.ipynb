{"cells":[{"cell_type":"markdown","metadata":{"id":"9037pIlOL2Av"},"source":["# Fine-tuning Llama2 for Mental Health Counseling\n","\n","The trained model is accessible on Huggingface under langecod/CounselLlama7B."]},{"cell_type":"markdown","metadata":{"id":"IGrFshwKMGST"},"source":["# 1.) Import & Install Necessary libraries (Colab requires installs with each run time)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:06:34.117648Z","iopub.status.busy":"2024-04-12T11:06:34.117100Z","iopub.status.idle":"2024-04-12T11:06:52.757800Z","shell.execute_reply":"2024-04-12T11:06:52.756601Z","shell.execute_reply.started":"2024-04-12T11:06:34.117621Z"},"id":"URszljNVkRtj","outputId":"f95a4f08-68ff-40de-b8b1-4e81864493a9","trusted":true},"outputs":[],"source":["pip install transformers datasets peft trl accelerate bitsandbytes packaging ninja sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:06:52.760528Z","iopub.status.busy":"2024-04-12T11:06:52.760152Z","iopub.status.idle":"2024-04-12T11:06:53.712982Z","shell.execute_reply":"2024-04-12T11:06:53.712070Z","shell.execute_reply.started":"2024-04-12T11:06:52.760480Z"},"id":"WmH4HNZgsfaT","outputId":"80a078bb-65df-4157-e32f-cfcfc7750018","trusted":true},"outputs":[],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:06:53.722125Z","iopub.status.busy":"2024-04-12T11:06:53.721867Z","iopub.status.idle":"2024-04-12T11:07:21.636199Z","shell.execute_reply":"2024-04-12T11:07:21.634923Z","shell.execute_reply.started":"2024-04-12T11:06:53.722104Z"},"id":"osnmxnzEnEm7","outputId":"e7837977-2891-4b3b-cdef-db4cca86375b","trusted":true},"outputs":[],"source":["pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:21.638520Z","iopub.status.busy":"2024-04-12T11:07:21.638070Z","iopub.status.idle":"2024-04-12T11:07:38.097629Z","shell.execute_reply":"2024-04-12T11:07:38.096773Z","shell.execute_reply.started":"2024-04-12T11:07:21.638484Z"},"id":"8ZE9nYjlj3V0","trusted":true},"outputs":[],"source":["import random\n","import gc\n","import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n","import numpy as np\n","import pandas as pd\n","import transformers\n","import accelerate\n","import bitsandbytes as bnb\n","from datasets import load_dataset, concatenate_datasets\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"xFbnC8UENnox"},"source":["# 2. Loading the Mental Health Conversations Dataset:\n","The Amod/mental_health_counseling_conversations dataset is a collection of 3512 questions and answer pairs sourced from counselchat.com, an online counseling and therapy platform. It covers a wide range of mental health topics with responses crafted from certified psychologists. It's tailored for refining language models, specifically for generating cogent advice on mental health inquiries. All entries pairs are in English, and each entry is structured with a 'Context' (user's question) and a 'Response' (psychologist's answer)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:38.108927Z","iopub.status.busy":"2024-04-12T11:07:38.108632Z","iopub.status.idle":"2024-04-12T11:07:41.497166Z","shell.execute_reply":"2024-04-12T11:07:41.496254Z","shell.execute_reply.started":"2024-04-12T11:07:38.108906Z"},"id":"cJe3t_PRj3V1","trusted":true},"outputs":[],"source":["dataset = load_dataset(\"nbertagnolli/counsel-chat\", split=\"train\")\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:41.498656Z","iopub.status.busy":"2024-04-12T11:07:41.498334Z","iopub.status.idle":"2024-04-12T11:07:41.885819Z","shell.execute_reply":"2024-04-12T11:07:41.884824Z","shell.execute_reply.started":"2024-04-12T11:07:41.498630Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","# Convert to DataFrame\n","df = pd.DataFrame(dataset)\n","\n","# Display the first few rows of the DataFrame\n","df.head(2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:41.887235Z","iopub.status.busy":"2024-04-12T11:07:41.886919Z","iopub.status.idle":"2024-04-12T11:07:42.229673Z","shell.execute_reply":"2024-04-12T11:07:42.228419Z","shell.execute_reply.started":"2024-04-12T11:07:41.887208Z"},"trusted":true},"outputs":[],"source":["\n","# Filter the required columns\n","#filtered_df = df[['questionText', 'topic']].drop_duplicates()\n","filtered_df = df[['questionText', 'topic', 'answerText']].drop_duplicates(subset=['questionText', 'topic'])\n","# Rename the columns\n","filtered_df.columns = ['Context', 'topic', 'Response']\n","\n","# Group by topic and count the occurrences\n","topic_counts = filtered_df['topic'].value_counts()\n","\n","# Calculate the target number of samples per topic for the test set\n","target_test_size_per_topic = (topic_counts * 0.2).round().astype(int)\n","\n","# Initialize an empty DataFrame for the test set\n","test_set_balanced = pd.DataFrame(columns=filtered_df.columns)\n","\n","# Initialize an empty DataFrame for the train set\n","train_set_balanced = pd.DataFrame(columns=filtered_df.columns)\n","\n","# For each topic, randomly select the calculated number of samples to include in the test set\n","for topic, target_size in target_test_size_per_topic.items():\n","    samples = filtered_df[filtered_df['topic'] == topic].sample(n=min(target_size, topic_counts[topic]), random_state=42)\n","    test_set_balanced = pd.concat([test_set_balanced, samples])\n","\n","        # Add the remaining samples to the train set\n","    train_set_balanced = pd.concat([train_set_balanced, df[df['topic'] == topic].drop(samples.index)])\n","\n","print(\"train data shape\",train_set_balanced.shape )\n","print(\"test data shape\",test_set_balanced.shape )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.231826Z","iopub.status.busy":"2024-04-12T11:07:42.231352Z","iopub.status.idle":"2024-04-12T11:07:42.478787Z","shell.execute_reply":"2024-04-12T11:07:42.477704Z","shell.execute_reply.started":"2024-04-12T11:07:42.231784Z"},"trusted":true},"outputs":[],"source":["\n","# Save the balanced test set\n","test_set_balanced.to_csv('counsel_chat_test_balanced.csv', index=False)\n","\n","# Save the balanced train set\n","train_set_balanced.to_csv('counsel_chat_train_balanced.csv', index=False)\n","\n","\n","# Check the final distribution of topics in the balanced test set\n","balanced_test_distribution = test_set_balanced['topic'].value_counts()\n","\n","print(balanced_test_distribution)\n","print(\"test data shape: \\n\",test_set_balanced.head() )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.484104Z","iopub.status.busy":"2024-04-12T11:07:42.483776Z","iopub.status.idle":"2024-04-12T11:07:42.554218Z","shell.execute_reply":"2024-04-12T11:07:42.553013Z","shell.execute_reply.started":"2024-04-12T11:07:42.484076Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(\"counsel_chat_train_balanced.csv\")\n","\n","# Filter the required columns\n","df = train_df[['questionText', 'answerText']]\n","# Rename the columns\n","df.columns = ['Context', 'Response']\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.566599Z","iopub.status.busy":"2024-04-12T11:07:42.566188Z","iopub.status.idle":"2024-04-12T11:07:42.672122Z","shell.execute_reply":"2024-04-12T11:07:42.671124Z","shell.execute_reply.started":"2024-04-12T11:07:42.566565Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# Assuming 'final_df' is your DataFrame loaded with pd.read_csv(\"formatted_data.csv\")\n","#train_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42)  # Splitting 20% for testing\n","\n","# Save the train and test datasets to CSV files\n","#test_df.to_csv('test_data.csv', index=False)\n","train_df = df.sample(frac=1, random_state=42)\n","\n","test_df = pd.read_csv(\"counsel_chat_test_balanced.csv\")\n","test_df1 = test_df[['Context', 'Response']]\n","\n","print(test_df.shape)\n","print(train_df.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.673631Z","iopub.status.busy":"2024-04-12T11:07:42.673303Z","iopub.status.idle":"2024-04-12T11:07:42.692314Z","shell.execute_reply":"2024-04-12T11:07:42.691504Z","shell.execute_reply.started":"2024-04-12T11:07:42.673603Z"},"trusted":true},"outputs":[],"source":["test_df1.to_csv('counsel_chat_train_balanced_one.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.693844Z","iopub.status.busy":"2024-04-12T11:07:42.693501Z","iopub.status.idle":"2024-04-12T11:07:42.833232Z","shell.execute_reply":"2024-04-12T11:07:42.832253Z","shell.execute_reply.started":"2024-04-12T11:07:42.693819Z"},"trusted":true},"outputs":[],"source":["\n","# If you want to save the new dataframe to a CSV file:\n","\n","train_df.to_csv('counsel_chat_train_balanced.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:42.834711Z","iopub.status.busy":"2024-04-12T11:07:42.834406Z","iopub.status.idle":"2024-04-12T11:07:43.250005Z","shell.execute_reply":"2024-04-12T11:07:43.249086Z","shell.execute_reply.started":"2024-04-12T11:07:42.834685Z"},"trusted":true},"outputs":[],"source":["\n","train = load_dataset(\"csv\", data_files=\"counsel_chat_train_balanced.csv\", split=\"train\")\n","#print(dataset[\"Text\"][400])\n","train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:43.251573Z","iopub.status.busy":"2024-04-12T11:07:43.251206Z","iopub.status.idle":"2024-04-12T11:07:43.564383Z","shell.execute_reply":"2024-04-12T11:07:43.563465Z","shell.execute_reply.started":"2024-04-12T11:07:43.251540Z"},"trusted":true},"outputs":[],"source":["test = load_dataset(\"csv\", data_files=\"counsel_chat_train_balanced_one.csv\" , split=\"train\")\n","#print(dataset[\"Text\"][400])\n","test"]},{"cell_type":"markdown","metadata":{"id":"PQ9W8ik2NvpJ"},"source":["# 3. Importing, Quantizing, and Preparing the Llama2 Chat Model:\n","To safeguard private health information and intellectual property, utilizing an open-sourced model is imperative. Meta's Llama 2 stands out in this regard, offering a collection of pretrained and fine-tuned large language models (LLMs) that span from 7 billion to 70 billion parameters. The Llama 2-Chat variant is especially tailored for dialogue applications, demonstrating superior performance over other open-source chat models in various benchmarks and human evaluations for both helpfulness and safety. This made the 7 billion parameter Llama 2-Chat model an ideal choice for our prototype. Additionally, to address memory constraints, expedite training, and ensure cost-effective operations, we employed a version of the model with 4-bit weights and activations through quantization."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:43.566024Z","iopub.status.busy":"2024-04-12T11:07:43.565676Z","iopub.status.idle":"2024-04-12T11:07:45.160600Z","shell.execute_reply":"2024-04-12T11:07:45.159364Z","shell.execute_reply.started":"2024-04-12T11:07:43.565992Z"},"id":"V1q5TitWj3V4","trusted":true},"outputs":[],"source":["nf4_config = BitsAndBytesConfig(\n","   load_in_4bit=True,\n","   bnb_4bit_quant_type=\"nf4\",\n","   bnb_4bit_use_double_quant=True,\n","   bnb_4bit_compute_dtype=torch.bfloat16 # A100\n",")\n","\n","#Load Tokenizer\n","tokenizer= AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token='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')\n","# Add Padding Token\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:07:45.162157Z","iopub.status.busy":"2024-04-12T11:07:45.161867Z","iopub.status.idle":"2024-04-12T11:08:50.652332Z","shell.execute_reply":"2024-04-12T11:08:50.651282Z","shell.execute_reply.started":"2024-04-12T11:07:45.162122Z"},"id":"5lSEwiWmIc5K","trusted":true},"outputs":[],"source":["# Load the LLaMA model in 4-bit\n","model = transformers.AutoModelForCausalLM.from_pretrained(\n","    \"meta-llama/Llama-2-7b-chat-hf\",\n","    token='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',\n","    quantization_config=nf4_config,\n","    use_flash_attention_2=False  #Improves attention algorithm from quadratic time down to linear\n",")"]},{"cell_type":"markdown","metadata":{"id":"uNgcUvcYomvj"},"source":["Using a 4-bit quantized model offers advantages in terms of memory usage, training speed, and inference performance. However, such quantization makes the model incompatible with conventional training approaches. To address this challenge, the \"Quantized Low-Rank Adaptation\" (QLoRA) method is employed. In QLoRA, the original pre-trained model weights remain frozen in 4-bit format, but an \"adapter\" with 16-bit model weights is created, allowing for fine-tuning on a specific task."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:08:50.654210Z","iopub.status.busy":"2024-04-12T11:08:50.653852Z","iopub.status.idle":"2024-04-12T11:08:50.670050Z","shell.execute_reply":"2024-04-12T11:08:50.669063Z","shell.execute_reply.started":"2024-04-12T11:08:50.654181Z"},"id":"KHHP8trFj3V4","trusted":true},"outputs":[],"source":["\n","import re\n","def get_num_layers(model):\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    return max(numbers)\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    return names\n","\n","peft_config = LoraConfig(\n","    r=8,\n","    lora_alpha=64,\n","    target_modules=get_last_layer_linears(model),\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:08:50.671696Z","iopub.status.busy":"2024-04-12T11:08:50.671276Z","iopub.status.idle":"2024-04-12T11:08:55.974850Z","shell.execute_reply":"2024-04-12T11:08:55.973719Z","shell.execute_reply.started":"2024-04-12T11:08:50.671668Z"},"id":"0hRVzTtKQPJb","trusted":true},"outputs":[],"source":["# prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{"id":"JYVuJK4bOJuv"},"source":["# 4.) Setting up the Trainer & Prompting:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:08:55.976754Z","iopub.status.busy":"2024-04-12T11:08:55.976380Z","iopub.status.idle":"2024-04-12T11:08:56.005356Z","shell.execute_reply":"2024-04-12T11:08:56.004163Z","shell.execute_reply.started":"2024-04-12T11:08:55.976717Z"},"id":"I3b_ZKCtEorx","trusted":true},"outputs":[],"source":["#pip install --upgrade torch transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:08:56.007164Z","iopub.status.busy":"2024-04-12T11:08:56.006738Z","iopub.status.idle":"2024-04-12T11:08:56.081738Z","shell.execute_reply":"2024-04-12T11:08:56.080586Z","shell.execute_reply.started":"2024-04-12T11:08:56.007124Z"},"id":"xOl4KN0Bj3V5","trusted":true},"outputs":[],"source":["#dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:10:02.636490Z","iopub.status.busy":"2024-04-12T11:10:02.636117Z","iopub.status.idle":"2024-04-12T11:10:02.642882Z","shell.execute_reply":"2024-04-12T11:10:02.641829Z","shell.execute_reply.started":"2024-04-12T11:10:02.636460Z"},"id":"Si8yEs_Bz4zt","trusted":true},"outputs":[],"source":["# System message to better instruct chatbot\n","'''\n","system_message = \"\"\"You are a helpful and and truthful psychology and psychotherapy assistant. Your primary role is to provide empathetic, understanding, and non-judgmental responses to users seeking emotional and psychological support.\n","                  Always respond with empathy and demonstrate active listening; try to focus on the user. Your responses should reflect that you understand the user's feelings and concerns. If a user expresses thoughts of self-harm, suicide, or harm to others, prioritize their safety.\n","                  Encourage them to seek immediate professional help and provide emergency contact numbers when appropriate.  You are not a licensed medical professional. Do not diagnose or prescribe treatments.\n","                  Instead, encourage users to consult with a licensed therapist or medical professional for specific advice. Avoid taking sides or expressing personal opinions. Your role is to provide a safe space for users to share and reflect.\n","                  Remember, your goal is to provide a supportive and understanding environment for users to share their feelings and concerns. Always prioritize their well-being and safety.\"\"\"\n","'''\n","system_message = \"\"\"You are supportive psychology and psychotherapy assistant, provide empathetic, non-judgmental responses, reflecting active listening and understanding of the user's emotions. Safety is paramount; You prioritize users' well-being, especially if they mention thoughts of self-harm, suicide, or harm to others.\"\"\"\n","\n","\n","\n","def format_llama(entry):\n","  formatted = f\"<s>[INST] <<SYS>>{system_message}<</SYS>>{entry['Context']} [/INST]  {entry['Response']}  </s>\"\n","\n","  return formatted"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:08:56.166373Z","iopub.status.busy":"2024-04-12T11:08:56.165959Z","iopub.status.idle":"2024-04-12T11:08:56.291451Z","shell.execute_reply":"2024-04-12T11:08:56.290479Z","shell.execute_reply.started":"2024-04-12T11:08:56.166338Z"},"id":"Vq0_xVP_1JZ2","trusted":true},"outputs":[],"source":["#dataset['train'][0]\n","train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:10:08.498407Z","iopub.status.busy":"2024-04-12T11:10:08.497498Z","iopub.status.idle":"2024-04-12T11:10:08.507421Z","shell.execute_reply":"2024-04-12T11:10:08.506400Z","shell.execute_reply.started":"2024-04-12T11:10:08.498374Z"},"id":"GSKB0uDN1Cmc","trusted":true},"outputs":[],"source":["#print(format_llama(dataset['train'][0]))\n","args = TrainingArguments(\n","    output_dir=\"CounselLlama7B\",\n","    logging_dir='./results',\n","    num_train_epochs=2,\n","    per_device_train_batch_size=1,\n","    gradient_checkpointing=True,\n","    optim=\"paged_adamw_8bit\",\n","    logging_steps=10,\n","    #save_strategy=\"epoch\",\n","    learning_rate=1e-4,\n","    tf32=False, #A100\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type=\"constant\",\n","    #load_best_model_at_end=True,\n","    #evaluation_strategy='epoch',\n","\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:10:12.778108Z","iopub.status.busy":"2024-04-12T11:10:12.777306Z","iopub.status.idle":"2024-04-12T11:10:15.044600Z","shell.execute_reply":"2024-04-12T11:10:15.043561Z","shell.execute_reply.started":"2024-04-12T11:10:12.778075Z"},"id":"XCqONHrPj3V5","trusted":true},"outputs":[],"source":["\n","\n","max_seq_length = 1024  # max sequence length for model and packing of the dataset\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train,\n","    #eval_dataset=val,\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_llama,\n","    args=args,\n","\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"Jy1N1rXjOXAf"},"source":["# 5.) Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:10:15.046587Z","iopub.status.busy":"2024-04-12T11:10:15.046289Z","iopub.status.idle":"2024-04-12T11:10:15.375222Z","shell.execute_reply":"2024-04-12T11:10:15.374171Z","shell.execute_reply.started":"2024-04-12T11:10:15.046561Z"},"id":"QmWIvE2m-fXl","trusted":true},"outputs":[],"source":["gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T11:10:15.376788Z","iopub.status.busy":"2024-04-12T11:10:15.376438Z","iopub.status.idle":"2024-04-12T15:03:03.760088Z","shell.execute_reply":"2024-04-12T15:03:03.759314Z","shell.execute_reply.started":"2024-04-12T11:10:15.376763Z"},"trusted":true},"outputs":[],"source":["# train        \n","trainer.train() \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:03:03.763013Z","iopub.status.busy":"2024-04-12T15:03:03.762671Z","iopub.status.idle":"2024-04-12T15:03:04.137750Z","shell.execute_reply":"2024-04-12T15:03:04.136595Z","shell.execute_reply.started":"2024-04-12T15:03:03.762988Z"},"trusted":true},"outputs":[],"source":["\n","# save model\n","trainer.save_model()\n","model.save_pretrained(\"llama2-finetuned\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T15:03:04.139557Z","iopub.status.busy":"2024-04-12T15:03:04.139156Z","iopub.status.idle":"2024-04-12T16:09:44.906733Z","shell.execute_reply":"2024-04-12T16:09:44.905596Z","shell.execute_reply.started":"2024-04-12T15:03:04.139500Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","n = 0\n","\n","model.config.use_cache = True\n","model.eval()\n","tokenizer= AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token='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')\n","# Add Padding Token\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, \n","                max_length=2048)\n","\n","# Placeholder for the generated responses\n","generated_responses = []\n","\n","test_df = pd.read_csv(\"counsel_chat_test_balanced.csv\")\n","test_df['Context'] = test_df['Context'].fillna('')\n","\n","for index, row in tqdm(test_df.iterrows()):\n","    prompt = f\"<s>[INST] <<SYS>>{system_message}<</SYS>>{row['Context']} [/INST]  </s>\"\n","    #prompt = f\"[INST] {row['Context'][:480]} [/INST]\"\n","    reference = row['Response']\n","    \n","    result = pipe(prompt)\n","    generated_text = result[0]['generated_text']\n","    #print(generated_text)\n","    \n","    generated_responses.append({\n","            'Context': row['Context'],\n","            'topic': row['topic'],\n","            'Response': generated_text\n","        })\n","    \n","\n","    if n < 5:\n","        print(\"Context:\",row['Context'])\n","        print(\"generated_text:\",generated_text)\n","        print(\"reference:\",reference)\n","    \n","    #if n > 5:\n","    #    break\n","      \n","    n +=1\n","    \n","# Create a DataFrame from the generated responses\n","generated_df = pd.DataFrame(generated_responses)\n","\n","# Write the DataFrame to an Excel file\n","generated_df.to_excel('Llama2_preds.xlsx', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:34:02.783676Z","iopub.status.busy":"2024-04-12T16:34:02.782711Z","iopub.status.idle":"2024-04-12T16:34:11.008805Z","shell.execute_reply":"2024-04-12T16:34:11.007587Z","shell.execute_reply.started":"2024-04-12T16:34:02.783634Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","# For saving PyTorch model\n","torch.save(model.state_dict(), \"llama2-finetuned.pth\")"]},{"cell_type":"markdown","metadata":{"id":"P-VTIIbFt3DX"},"source":["# Convert Test Dataset to Test DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:09:44.909108Z","iopub.status.busy":"2024-04-12T16:09:44.908298Z","iopub.status.idle":"2024-04-12T16:09:44.931605Z","shell.execute_reply":"2024-04-12T16:09:44.930630Z","shell.execute_reply.started":"2024-04-12T16:09:44.909067Z"},"id":"0g2Q1PIEt0nd","trusted":true},"outputs":[],"source":["# Convert to DataFrame\n","test_df = pd.DataFrame(test)\n","\n","# Display the first few rows of the DataFrame\n","test_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"Q1z-8VPmPxzc"},"source":["# 6.) Chatbot User Interface:\n","\n","A simple chatbot user interface setuped so that the user can interact with the model, ask mental health related questions, and sample the responses. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:12:42.167287Z","iopub.status.busy":"2024-04-12T16:12:42.166531Z","iopub.status.idle":"2024-04-12T16:12:42.183721Z","shell.execute_reply":"2024-04-12T16:12:42.182806Z","shell.execute_reply.started":"2024-04-12T16:12:42.167252Z"},"id":"MThcmWAzIM1V","trusted":true},"outputs":[],"source":["from IPython.core.display import display, HTML\n","from ipywidgets import widgets, Layout, Box\n","from IPython.display import clear_output\n","\n","text_input = widgets.Textarea(\n","    value='',\n","    placeholder='Type your message here...',\n","    description='Input:',\n","    disabled=False,\n","    layout=Layout(width='38.2%')\n",")\n","\n","button = widgets.Button(description=\"Submit\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:12:45.251587Z","iopub.status.busy":"2024-04-12T16:12:45.250963Z","iopub.status.idle":"2024-04-12T16:12:45.276914Z","shell.execute_reply":"2024-04-12T16:12:45.275728Z","shell.execute_reply.started":"2024-04-12T16:12:45.251545Z"},"id":"6JXHZ40cITlb","trusted":true},"outputs":[],"source":["output_area = widgets.Output(layout=Layout(width='61.8%'))\n","\n","# Add a processing indication label below your text_input\n","processing_label = widgets.Label(value='')  # Initialize with an empty value\n","\n","# System message to better instruct chatbot\n","system_message = \"\"\"You are a helpful and and truthful psychology and psychotherapy assistant. Your primary role is to provide empathetic, understanding, and non-judgmental responses to users seeking emotional and psychological support.\n","                  Always respond with empathy and demonstrate active listening; try to focus on the user. Your responses should reflect that you understand the user's feelings and concerns. If a user expresses thoughts of self-harm, suicide, or harm to others, prioritize their safety.\n","                  Encourage them to seek immediate professional help and provide emergency contact numbers when appropriate.  You are not a licensed medical professional. Do not diagnose or prescribe treatments.\n","                  Instead, encourage users to consult with a licensed therapist or medical professional for specific advice. Avoid taking sides or expressing personal opinions. Your role is to provide a safe space for users to share and reflect.\n","                  Remember, your goal is to provide a supportive and understanding environment for users to share their feelings and concerns. Always prioritize their well-being and safety.\"\"\"\n","\n","# Display Greeting Message\n","with output_area:\n","  display(HTML(f'<strong>Assistant: </strong>Hi there! How are you today?'))\n","  display(HTML('<br/><br/>'))\n","\n","def on_submit_button_clicked(b):\n","    with output_area:\n","        # Get user input\n","        user_input = text_input.value\n","        formatted = f\"<s>[INST] <<SYS>>{system_message}<</SYS>>{user_input} [/INST]\"\n","        # Display input\n","        display(HTML(f'<strong>User:</strong> {user_input}'))\n","        display(HTML('<br/><br/>'))\n","\n","        # Show processing indication\n","        processing_label.value = 'Processing...'\n","\n","        # Use your chatbot model to get a response\n","        input_ids = tokenizer(formatted, return_tensors=\"pt\", truncation=True, max_length=2048).input_ids.cuda()\n","        # with torch.inference_mode():\n","        outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9,temperature=0.95)\n","        translated_output=tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(formatted)-1:]\n","\n","        # Display response (characters added to bold User/Assistant)\n","        display(HTML(f'<strong>Assistant:</strong> {translated_output}'))\n","        display(HTML('<br/><br/>'))\n","\n","        # Clear the processing indication\n","        processing_label.value = ''\n","\n","        # Clear the text input\n","        text_input.value = ''\n","\n","button.on_click(on_submit_button_clicked)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T16:12:49.188553Z","iopub.status.busy":"2024-04-12T16:12:49.188151Z","iopub.status.idle":"2024-04-12T16:12:49.201561Z","shell.execute_reply":"2024-04-12T16:12:49.200554Z","shell.execute_reply.started":"2024-04-12T16:12:49.188500Z"},"id":"ysBNFKimIgv_","trusted":true},"outputs":[],"source":["# Display widgets\n","display(text_input, button, processing_label, output_area)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:09:58.340130Z","iopub.status.idle":"2024-04-12T16:09:58.340472Z","shell.execute_reply":"2024-04-12T16:09:58.340307Z","shell.execute_reply.started":"2024-04-12T16:09:58.340294Z"},"id":"cHUjEH9BuML5","trusted":true},"outputs":[],"source":["clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:09:58.341811Z","iopub.status.idle":"2024-04-12T16:09:58.342154Z","shell.execute_reply":"2024-04-12T16:09:58.342005Z","shell.execute_reply.started":"2024-04-12T16:09:58.341990Z"},"id":"XFoPYbcD-afB","trusted":true},"outputs":[],"source":["# Push model to hub since Google colab empties out directory\n","from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:09:58.343694Z","iopub.status.idle":"2024-04-12T16:09:58.344044Z","shell.execute_reply":"2024-04-12T16:09:58.343892Z","shell.execute_reply.started":"2024-04-12T16:09:58.343877Z"},"id":"N-4QhSM2FFk0","trusted":true},"outputs":[],"source":["trainer.push_to_hub('')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T16:09:58.345953Z","iopub.status.idle":"2024-04-12T16:09:58.346298Z","shell.execute_reply":"2024-04-12T16:09:58.346147Z","shell.execute_reply.started":"2024-04-12T16:09:58.346133Z"},"id":"r8qOZJ5B8_wl","trusted":true},"outputs":[],"source":["pip freeze > requirements.txt\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4086571,"sourceId":7091481,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
